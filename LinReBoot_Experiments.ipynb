{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Bandit():\n",
    "    '''\n",
    "    stochastic linear (contextual) bandit environment\n",
    "    rewards are Gaussian\n",
    "    ============================================\n",
    "    k: number of arms (int)\n",
    "    n: length of horizon (int)\n",
    "    sigma: reward is sigma^2-SubGaussian, list of positive float with length k\n",
    "    beta: linear coefficient for mean reward for each arm, true parameter for linear bandit\n",
    "          k by d(or 1 by d) numpy array for user-defined values.\n",
    "    random_context: True if contexts are stochastic/generated from some distribution,\n",
    "                    False if contexts are deterministic/fixed.\n",
    "    gen_context: function to generate d-dimensional context for all arms, output k by d numpy array\n",
    "    '''\n",
    "    def  __init__(self, k, n, beta, sigma, random_context = True, gen_context = None):\n",
    "        \n",
    "        self.k = k                                  # number of arms\n",
    "        self.n = n                                  # length of horizon\n",
    "        self.sigma = sigma                          # SubGaussian constants\n",
    "        self.beta = np.array(beta)                  # linear coefficient, parameter for linear bandit\n",
    "        self.d = beta.shape[1]                      # dimension of context                 \n",
    "        \n",
    "        if gen_context is None:\n",
    "            print(\"error: please specify a funtion for generating context\")\n",
    "            \n",
    "        # make tables\n",
    "        if beta.shape[0] == 1:\n",
    "            ## This is the case that true coefficient is shared by arms\n",
    "            betas = np.repeat(beta, k, axis = 0)\n",
    "        else:\n",
    "            ## This is the case that coefficients are different among arms(linear bandit with covariates)\n",
    "            betas = beta\n",
    "        \n",
    "        d = beta.shape[1]\n",
    "        reward_table = np.zeros(k*n).reshape(n ,k)\n",
    "        context_table = np.zeros(d*n*k).reshape(n, k, d)\n",
    "        mu_table = np.zeros(k*n).reshape(n, k)\n",
    "        if random_context:\n",
    "            ## random context, context is generated at each time\n",
    "            for i in range(n):\n",
    "                c = gen_context(k = k, d = d)\n",
    "                context_table[i,:,:] = c\n",
    "                for j in range(k):\n",
    "                    mu_table[i,j] = np.inner(betas[j,:], c[j,:])\n",
    "                reward_table[i,:] = stats.multivariate_normal.rvs(mean = mu_table[i,:], cov = np.diag(sigma**2))\n",
    "        else:\n",
    "            ## fixed context, context is fixed along time\n",
    "            c = gen_context(k = k, d = d)\n",
    "            mu = np.zeros(k).reshape(k)\n",
    "            for j in range(k):\n",
    "                mu[j] = np.inner(betas[j,:], c[j,:])\n",
    "            for i in range(n):\n",
    "                context_table[i,:,:] = c\n",
    "                mu_table[i,:] = mu\n",
    "                reward_table[i,:] = stats.multivariate_normal.rvs(mean = mu, cov = np.diag(sigma**2))\n",
    "        \n",
    "        self.rewards = reward_table                       # reward table, n by k array\n",
    "        self.contexts = context_table                     # context table, n by k by d array\n",
    "        self.mus = mu_table                               # mu table, n by k array\n",
    "        self.random_arms = np.random.randint(1, k+1, n)   # random arms sequence with length n for arm-independent context \n",
    "\n",
    "    def pull(self, a, t):\n",
    "        '''\n",
    "        pull arm/take action and observe reward\n",
    "        ============================================\n",
    "        INPUT\n",
    "            a: action\n",
    "            t: time\n",
    "        ============================================\n",
    "        OUPUT\n",
    "            r: reward\n",
    "        '''\n",
    "        r = self.rewards[t,a-1]\n",
    "        return r\n",
    "        \n",
    "    \n",
    "    def get_context(self, t, all_arm = True):\n",
    "        '''\n",
    "        get context\n",
    "        ============================================\n",
    "        INPUT\n",
    "            t: time\n",
    "            all_arm: True return k by d array for k contexts; False return one d by 1 context randomly\n",
    "        ============================================\n",
    "        OUPUT\n",
    "            c: context, k by d or d by 1\n",
    "        '''\n",
    "        if not all_arm:\n",
    "            c = self.contexts[t, self.random_arms[t]-1, :].reshape(self.d, 1) \n",
    "        else:\n",
    "            c = self.contexts[t, :, :]\n",
    "        return c\n",
    "    \n",
    "    def regret(self, a, t):\n",
    "        '''\n",
    "        regret for current action\n",
    "        ============================================\n",
    "        INPUT\n",
    "            a: action\n",
    "            t: time\n",
    "        ============================================\n",
    "        OUPUT\n",
    "            regret: regret\n",
    "        '''\n",
    "        mu = self.mus[t,:]\n",
    "        mu_best = max(mu)\n",
    "        regret = mu_best - mu[a-1]\n",
    "        return regret\n",
    "    \n",
    "    def beta_sharing(self):\n",
    "        '''\n",
    "        indicator for beta sharing among arms\n",
    "        ============================================\n",
    "        INPUT\n",
    "        ============================================\n",
    "        OUPUT\n",
    "            sharing: True if beta is same among arms\n",
    "        '''\n",
    "        return self.beta.shape[0] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_ReBoot_G(env, lam, weight_sd, coefficient_sharing = True):\n",
    "    '''\n",
    "    Gaussian Residual Boostrap Exploration assuming linear contextual bandit\n",
    "    This is a linear bandit algorithm\n",
    "    ============================================\n",
    "    INPUTS\n",
    "        env: stochastic bandit environment\n",
    "        lam: regularization parameter\n",
    "        weight_sd: standard deviation of residual bootstrap weights\n",
    "        coefficient sharing: True if assuming true coefficient is same among arms\n",
    "    ============================================\n",
    "    OUPUTS \n",
    "        R: reward sequence, list with length n\n",
    "        A is action sequence, list with length n\n",
    "        regret: regret sequence, list with length n\n",
    "    '''\n",
    "    # set up\n",
    "    n = env.n\n",
    "    K = env.k\n",
    "    d = env.d\n",
    "    lam = lam + 1e-20\n",
    "    regret = [0]\n",
    "    A = []\n",
    "    R = []\n",
    "    \n",
    "    if not coefficient_sharing:\n",
    "        beta_est = np.zeros(d*K).reshape(d, K)\n",
    "        V_est = [np.identity(d)*lam for i in range(K)]\n",
    "        g = [np.zeros(d).reshape(d,1) for i in range(K)]\n",
    "        arm_count = np.zeros(K)\n",
    "        Sum1_by_A = np.zeros(K)\n",
    "        Sum2_by_A = np.zeros(K)\n",
    "        Y_by_A = [np.empty((0,1))]*K\n",
    "        X_by_A = [np.empty((0,d))]*K\n",
    "        \n",
    "        # temporary liat/array\n",
    "        mu_est = np.zeros(K)\n",
    "        \n",
    "        # pull each arm once\n",
    "        for t in range(1, K+1):\n",
    "            a_t = t\n",
    "            c_t = env.get_context(t, all_arm = False)\n",
    "            r_t = env.pull(a_t, t)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y_by_A[a_t - 1] = np.append(Y_by_A[a_t - 1], np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X_by_A[a_t - 1] = np.append(X_by_A[a_t - 1], np.array(c_t).reshape(1, d), axis = 0)\n",
    "            ##incremental update\n",
    "            V_est[a_t - 1] = V_est[a_t - 1] + np.matmul(c_t, c_t.T)\n",
    "            g[a_t - 1] = g[a_t - 1] + r_t * c_t.reshape(d,1)\n",
    "            Sum1_by_A[a_t - 1] = Sum1_by_A[a_t - 1] + r_t\n",
    "            Sum2_by_A[a_t - 1] = Sum2_by_A[a_t - 1] + r_t**2\n",
    "            arm_count[a_t - 1] = arm_count[a_t - 1] + 1\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "            \n",
    "        # ReBoot loop\n",
    "        for t in range(K+1, n):\n",
    "            ## LSE update\n",
    "            X = X_by_A[a_t - 1]\n",
    "            Y = Y_by_A[a_t - 1]\n",
    "            beta_est[:,a_t - 1] = (np.linalg.inv(V_est[a_t - 1]) @ g[a_t - 1]).reshape(d)\n",
    "\n",
    "            \n",
    "            ## ReBoot exploration\n",
    "            c_t = env.get_context(t, all_arm = False)\n",
    "            mu_hat = np.matmul(c_t.T, beta_est).reshape(K)\n",
    "            Sigma_diag = (Sum2_by_A + arm_count * mu_hat * mu_hat - 2 * mu_hat * mu_hat * Sum1_by_A)/(arm_count*arm_count) \n",
    "            mu_est = stats.multivariate_normal.rvs(size = 1, mean = mu_hat, cov = np.diag(weight_sd**2 * Sigma_diag))\n",
    "            \n",
    "            ## pull arm\n",
    "            a_t = np.argmax(mu_est) + 1\n",
    "            r_t = env.pull(a_t, t)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y_by_A[a_t - 1] = np.append(Y_by_A[a_t - 1], np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X_by_A[a_t - 1] = np.append(X_by_A[a_t - 1], np.array(c_t).reshape(1, d), axis = 0)\n",
    "            \n",
    "            ## incremental update\n",
    "            V_est[a_t - 1] = V_est[a_t - 1] + np.matmul(c_t, c_t.T)\n",
    "            g[a_t - 1] = g[a_t - 1] + r_t * c_t.reshape(d,1)\n",
    "            Sum1_by_A[a_t - 1] = Sum1_by_A[a_t - 1] + r_t\n",
    "            Sum2_by_A[a_t - 1] = Sum2_by_A[a_t - 1] + r_t**2\n",
    "            arm_count[a_t - 1] = arm_count[a_t - 1] + 1\n",
    "\n",
    "            ## compute regret\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "            \n",
    "    else:\n",
    "        beta_est = np.zeros(d).reshape(d, 1)\n",
    "        V_est = np.identity(d)*lam\n",
    "        g = np.zeros(d).reshape(d,1)\n",
    "        arm_count = np.zeros(K)\n",
    "        Sum1_by_A = np.zeros(K)\n",
    "        Sum2_by_A = np.zeros(K)\n",
    "        Y = np.empty((0,1))\n",
    "        X = np.empty((0,d))\n",
    "        \n",
    "        # pull each arm once\n",
    "        for t in range(1, K+1):\n",
    "            a_t = t\n",
    "            r_t = env.pull(a_t, t)\n",
    "            c_K = env.get_context(t, True)\n",
    "            c_t = c_K[a_t - 1,:].reshape(d,1)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y = np.append(Y, np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X = np.append(X, np.array(c_t).reshape(1, d), axis = 0)\n",
    "            V_est = V_est + np.matmul(c_t, c_t.T)\n",
    "            g = g + r_t * c_t.reshape(d,1)\n",
    "            Sum1_by_A[a_t - 1] = Sum1_by_A[a_t - 1] + r_t\n",
    "            Sum2_by_A[a_t - 1] = Sum2_by_A[a_t - 1] + r_t**2\n",
    "            arm_count[a_t - 1] = arm_count[a_t - 1] + 1\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "            \n",
    "        # ReBoot loop\n",
    "        for t in range(K+1, n):\n",
    "            ## LSE update\n",
    "            beta_est = np.linalg.inv(V_est) @ g\n",
    "            \n",
    "            ## ReBoot exploration\n",
    "            c_K = env.get_context(t, True)\n",
    "            mu_hat = np.matmul(c_K, beta_est).reshape(K)\n",
    "            Sigma_diag = (Sum2_by_A + arm_count * mu_hat * mu_hat - 2 * mu_hat * mu_hat * Sum1_by_A)/(arm_count*arm_count) \n",
    "            mu_est = stats.multivariate_normal.rvs(size = 1, mean = mu_hat, cov = np.diag(weight_sd**2 * Sigma_diag))\n",
    "            \n",
    "            ## pull arm\n",
    "            a_t = np.argmax(mu_est) + 1\n",
    "            r_t = env.pull(a_t, t)\n",
    "            c_t = c_K[a_t - 1,:].reshape(d,1)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y = np.append(Y, np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X = np.append(X, np.array(c_t).reshape(1, d), axis = 0)\n",
    "            \n",
    "            # incremental update\n",
    "            V_est = V_est + np.matmul(c_t, c_t.T)\n",
    "            g = g + r_t * c_t.reshape(d,1)\n",
    "            Sum1_by_A[a_t - 1] = Sum1_by_A[a_t - 1] + r_t\n",
    "            Sum2_by_A[a_t - 1] = Sum2_by_A[a_t - 1] + r_t**2\n",
    "            arm_count[a_t - 1] = arm_count[a_t - 1] + 1\n",
    "\n",
    "            ## compute regret\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "            \n",
    "    return R, A, regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_TS_G(env, tau, coefficient_sharing = True):\n",
    "    '''\n",
    "    Linear Thompson Sampling with Gaussian prior N(0, (tau^2)*I)\n",
    "    Same piror for each arm\n",
    "    This is a linear bandit algorithm\n",
    "    ============================================\n",
    "    INPUTS\n",
    "        env: stochastic linear bandit environment\n",
    "        tau: std in prior (positive float)\n",
    "        coefficient sharing: True if assuming true coefficient is same among arms\n",
    "    ============================================\n",
    "    OUPUTS \n",
    "        R: reward sequence, list with length n\n",
    "        A is action sequence, list with length n\n",
    "        regret: regret sequence, list with length n\n",
    "    ''' \n",
    "    # set up\n",
    "    n = env.n\n",
    "    K = env.k\n",
    "    d = env.d\n",
    "    regret = [0]\n",
    "    A = []\n",
    "    R = []\n",
    "    \n",
    "    if not coefficient_sharing:\n",
    "        beta_mean = [np.zeros(d) for i in range(K)]\n",
    "        beta_V = [np.identity(d)*(1/tau**2) for i in range(K)]\n",
    "        beta_sigma = [np.identity(d)*tau**2 for i in range(K)]\n",
    "        beta_g = [np.zeros(d).reshape(d,1) for i in range(K)]\n",
    "        sigma_est = np.ones(K)\n",
    "        Y_by_A = [np.empty((0,1))]*K\n",
    "        X_by_A = [np.empty((0,d))]*K\n",
    "\n",
    "\n",
    "        # temporary liat/array\n",
    "        beta_est = np.zeros(K*d).reshape(K,d)\n",
    "\n",
    "        # Thompson Sampling loop\n",
    "        for t in range(1, n):\n",
    "            ## estimation\n",
    "            c_t = env.get_context(t, all_arm = False)\n",
    "            for j in range(K):\n",
    "                beta_est[j,:]  = stats.multivariate_normal.rvs(mean = beta_mean[j].reshape(d), cov = (sigma_est[j]**2)*beta_sigma[j])\n",
    "            c_t = np.array(c_t).reshape(d, 1)\n",
    "            mu_est = np.matmul(beta_est, c_t).reshape(K)\n",
    "\n",
    "            ## pull arm\n",
    "            a_t = np.argmax(mu_est) + 1\n",
    "            r_t = env.pull(a_t, t)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y_by_A[a_t - 1] = np.append(Y_by_A[a_t - 1], np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X_by_A[a_t - 1] = np.append(X_by_A[a_t - 1], np.array(c_t).reshape(1, d), axis = 0)\n",
    "\n",
    "            ## update\n",
    "            X = X_by_A[a_t - 1]\n",
    "            Y = Y_by_A[a_t - 1]\n",
    "            sigma_est[a_t - 1] = np.std(Y)\n",
    "            beta_V[a_t - 1] = beta_V[a_t - 1] + np.matmul(c_t, c_t.T)\n",
    "            beta_sigma[a_t - 1] = np.linalg.inv(beta_V[a_t - 1])\n",
    "            beta_g[a_t - 1] = beta_g[a_t - 1] + r_t * c_t.reshape(d,1)\n",
    "            beta_mean[a_t - 1] = np.matmul(beta_sigma[a_t - 1], beta_g[a_t - 1])\n",
    "\n",
    "\n",
    "            ## compute regret\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "    \n",
    "    else:\n",
    "        beta_mean = np.zeros(d)\n",
    "        beta_V = np.identity(d)*(1/tau**2)\n",
    "        beta_sigma = np.identity(d)*tau**2\n",
    "        beta_g = np.zeros(d).reshape(d,1)\n",
    "        sigma_est = np.ones(1)\n",
    "        Y = np.empty((0,1))\n",
    "        X = np.empty((0,d))\n",
    "\n",
    "        # pull each arm once\n",
    "        for t in range(1, K+1):\n",
    "            a_t = t\n",
    "            r_t = env.pull(a_t, t)\n",
    "            c_K = env.get_context(t, True)\n",
    "            c_t = c_K[a_t - 1,:].reshape(d,1)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y = np.append(Y, np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X = np.append(X, np.array(c_t).reshape(1, d), axis = 0)\n",
    "            sigma_est = np.std(Y)\n",
    "            beta_V = beta_V + np.matmul(c_t, c_t.T)\n",
    "            beta_sigma = np.linalg.inv(beta_V)\n",
    "            beta_g = beta_g + r_t * c_t.reshape(d,1)\n",
    "            beta_mean = np.matmul(beta_sigma, beta_g)\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "        \n",
    "        # Thompson Sampling loop\n",
    "        for t in range(K+1, n):            \n",
    "            ## estimation\n",
    "            c_K = env.get_context(t, True)\n",
    "            beta_est  = stats.multivariate_normal.rvs(mean = beta_mean.reshape(d), cov = (sigma_est**2)*beta_sigma)\n",
    "            mu_est = np.matmul(c_K, beta_est).reshape(K)\n",
    "        \n",
    "            ## pull arm\n",
    "            a_t = np.argmax(mu_est) + 1\n",
    "            r_t = env.pull(a_t, t)\n",
    "            c_t = c_K[a_t - 1,:].reshape(d,1)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y = np.append(Y, np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X = np.append(X, np.array(c_t).reshape(1, d), axis = 0)\n",
    "\n",
    "            ## update\n",
    "            sigma_est = np.std(Y)\n",
    "            beta_V = beta_V + np.matmul(c_t, c_t.T)\n",
    "            beta_sigma = np.linalg.inv(beta_V)\n",
    "            beta_g = beta_g + r_t * c_t.reshape(d,1)\n",
    "            beta_mean = np.matmul(beta_sigma, beta_g)\n",
    "\n",
    "            ## compute regret\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "        \n",
    "    return R, A, regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_TS_IG(env, tau, alpha, coefficient_sharing = True):\n",
    "    '''\n",
    "    Linear Thompson Sampling with priors N(0, (tau^2)*I) and IG(alpha, alpha)\n",
    "    Same piror for each arm\n",
    "    This is a linear bandit algorithm\n",
    "    ============================================\n",
    "    INPUTS\n",
    "        env: stochastic linear bandit environment\n",
    "        tau: std in Gaussian prior (positive float)\n",
    "        alpha: parameter in Inverse Gamma prior\n",
    "        coefficient sharing: True if assuming true coefficient is same among arms\n",
    "    ============================================\n",
    "    OUPUTS \n",
    "        R: reward sequence, list with length n\n",
    "        A is action sequence, list with length n\n",
    "        regret: regret sequence, list with length n\n",
    "    ''' \n",
    "    # set up\n",
    "    n = env.n\n",
    "    K = env.k\n",
    "    d = env.d\n",
    "    regret = [0]\n",
    "    A = []\n",
    "    R = []\n",
    "    \n",
    "    if not coefficient_sharing:\n",
    "        beta_mean = [np.zeros(d) for i in range(K)]\n",
    "        beta_V = [np.identity(d)*(1/tau**2) for i in range(K)]\n",
    "        beta_sigma = [np.identity(d)*tau**2 for i in range(K)]\n",
    "        beta_g = [np.zeros(d).reshape(d,1) for i in range(K)]\n",
    "        eta_a = np.ones(K)*alpha\n",
    "        eta_b = np.ones(K)*alpha\n",
    "        sigma_est = np.ones(K)\n",
    "        Y_by_A = [np.empty((0,1))]*K\n",
    "        X_by_A = [np.empty((0,d))]*K\n",
    "\n",
    "\n",
    "        # temporary liat/array\n",
    "        beta_est = np.zeros(K*d).reshape(K,d)\n",
    "\n",
    "        # Thompson Sampling loop\n",
    "        for t in range(1, n):\n",
    "            c_t = env.get_context(t, False)\n",
    "            for j in range(K):\n",
    "                sigma_est[j] = scipy.stats.invgamma.rvs(a=eta_a[j], scale=eta_b[j])\n",
    "                beta_est[j,:]  = stats.multivariate_normal.rvs(mean = beta_mean[j].reshape(d), cov = sigma_est[j]*beta_sigma[j])\n",
    "            c_t = np.array(c_t).reshape(d, 1)\n",
    "            mu_est = np.matmul(beta_est, c_t).reshape(K)\n",
    "\n",
    "            ## pull arm\n",
    "            a_t = np.argmax(mu_est) + 1\n",
    "            r_t = env.pull(a_t, t)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y_by_A[a_t - 1] = np.append(Y_by_A[a_t - 1], np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X_by_A[a_t - 1] = np.append(X_by_A[a_t - 1], np.transpose(c_t), axis = 0)\n",
    "\n",
    "            ## update\n",
    "            X = X_by_A[a_t - 1]\n",
    "            Y = Y_by_A[a_t - 1]\n",
    "            beta_V[a_t - 1] = beta_V[a_t - 1] + np.matmul(c_t, c_t.T)\n",
    "            beta_sigma[a_t - 1] = np.linalg.inv(beta_V[a_t - 1])\n",
    "            beta_g[a_t - 1] = beta_g[a_t - 1] + r_t * c_t.reshape(d,1)\n",
    "            beta_mean[a_t - 1] = np.matmul(beta_sigma[a_t - 1], beta_g[a_t - 1])\n",
    "            eta_a[a_t - 1] = eta_a[a_t - 1] + 1/2\n",
    "            eta_b[a_t - 1] = alpha + 0.5*np.matmul(beta_g[a_t - 1].T, beta_mean[a_t - 1])\n",
    "\n",
    "            ## compute regret\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "\n",
    "    \n",
    "    else:\n",
    "        beta_mean = np.zeros(d)\n",
    "        beta_V = np.identity(d)*(1/tau**2)\n",
    "        beta_sigma = np.identity(d)*tau**2\n",
    "        beta_g = np.zeros(d).reshape(d,1)\n",
    "        eta_a = np.ones(1)*alpha\n",
    "        eta_b = np.ones(1)*alpha\n",
    "        sigma_est = np.ones(1)\n",
    "        Y = np.empty((0,1))\n",
    "        X = np.empty((0,d))\n",
    "        X_K = np.zeros(d*K).reshape(K,d)\n",
    "        \n",
    "        # pull each arm once\n",
    "        for t in range(1, K+1):\n",
    "            a_t = t\n",
    "            r_t = env.pull(a_t, t)\n",
    "            c_K = env.get_context(t, True)\n",
    "            c_t = c_K[a_t - 1,:].reshape(d,1)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y = np.append(Y, np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X = np.append(X, np.array(c_t).reshape(1, d), axis = 0)\n",
    "            beta_V = beta_V + np.matmul(c_t, c_t.T)\n",
    "            beta_sigma = np.linalg.inv(beta_V)\n",
    "            beta_g = beta_g + r_t * c_t.reshape(d,1)\n",
    "            beta_mean = np.matmul(beta_sigma, beta_g)\n",
    "            eta_a = eta_a + 1/2\n",
    "            eta_b = alpha + 0.5*np.matmul(beta_g.T, beta_mean)\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "        \n",
    "        # Thompson Sampling loop\n",
    "        for t in range(K+1, n):         \n",
    "            ## estimation\n",
    "            c_K = env.get_context(t, True)\n",
    "            sigma_est = scipy.stats.invgamma.rvs(a=eta_a, scale=eta_b)\n",
    "            beta_est  = stats.multivariate_normal.rvs(mean = beta_mean.reshape(d), cov = (sigma_est**2)*beta_sigma)\n",
    "            mu_est = np.matmul(c_K, beta_est).reshape(K)\n",
    "        \n",
    "            ## pull arm\n",
    "            a_t = np.argmax(mu_est) + 1\n",
    "            r_t = env.pull(a_t, t)\n",
    "            c_t = c_K[a_t - 1,:].reshape(d,1)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y = np.append(Y, np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X = np.append(X, np.array(c_t).reshape(1, d), axis = 0)\n",
    "            \n",
    "            # update\n",
    "            beta_V = beta_V + np.matmul(c_t, c_t.T)\n",
    "            beta_sigma = np.linalg.inv(beta_V)\n",
    "            beta_g = beta_g + r_t * c_t.reshape(d,1)\n",
    "            beta_mean = np.matmul(beta_sigma, beta_g)\n",
    "            eta_a = eta_a + 1/2\n",
    "            eta_b = alpha + 0.5*np.matmul(beta_g.T, beta_mean)\n",
    "            \n",
    "            ## compute regret\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "        \n",
    "    return R, A, regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_GIRO(env, a, lam, R_upper, R_lower, coefficient_sharing = True):\n",
    "    '''\n",
    "    Garbage In, Reward Out Algorithm\n",
    "    Boostrap exploration for bounded reward\n",
    "    This is a linear bandit algorithm\n",
    "    ============================================\n",
    "    INPUTS\n",
    "        env: stochastic bandit environment\n",
    "        a: number of postive/negative pseudo rewards per time unit (int)\n",
    "        lam: regularization parameter \n",
    "        R_upper: upper bound for reward  \n",
    "        R_lower: lower bound for reward\n",
    "        coefficient sharing: True if assuming true coefficient is same among arms\n",
    "    ============================================\n",
    "    OUPUTS \n",
    "        R: reward sequence, list with length n\n",
    "        A is action sequence, list with length n\n",
    "        regret: regret sequence, list with length n\n",
    "    '''\n",
    "    # set up\n",
    "    n = env.n\n",
    "    K = env.k\n",
    "    d = env.d\n",
    "    lam = lam + 1e-20\n",
    "    regret = [0]\n",
    "    A = []\n",
    "    R = []\n",
    "    \n",
    "    if not coefficient_sharing:\n",
    "        beta_est = [np.zeros(d) for i in range(K)]\n",
    "        V_est = [np.identity(d)*lam for i in range(K)]\n",
    "        Y_by_A = [np.empty((0,1))]*K\n",
    "        X_by_A = [np.empty((0,d))]*K\n",
    "\n",
    "        # temporary liat/array\n",
    "        mu_est = np.zeros(K)\n",
    "\n",
    "        # GIRO loop\n",
    "        for t in range(1, n):\n",
    "            c_t = env.get_context(t, False)\n",
    "            c_t = np.array(c_t).reshape(d, 1)\n",
    "            for k in range(K):\n",
    "                s = len(Y_by_A[k])\n",
    "                ## Boostrapping\n",
    "                idx = np.random.choice(s,s)\n",
    "                X = X_by_A[k][idx,:]\n",
    "                Y = Y_by_A[k][idx,:]\n",
    "                V_est[k] = np.linalg.inv(np.matmul(np.transpose(X), X) + np.identity(d)*lam)\n",
    "                beta_est[k] = np.matmul(V_est[k], np.matmul(np.transpose(X), Y))\n",
    "                mu_est[k] = np.matmul(np.transpose(beta_est[k]), c_t)\n",
    "\n",
    "            ## pull arm\n",
    "            a_t = np.argmax(mu_est) + 1\n",
    "            r_t = env.pull(a_t, t)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y_by_A[a_t - 1] = np.append(Y_by_A[a_t - 1], np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X_by_A[a_t - 1] = np.append(X_by_A[a_t - 1], np.transpose(c_t), axis = 0)\n",
    "\n",
    "            ## pseudo rewards\n",
    "            for i in range(a):\n",
    "                Y_by_A[a_t - 1] = np.append(Y_by_A[a_t - 1], np.array(R_upper).reshape(1, 1), axis = 0)\n",
    "                X_by_A[a_t - 1] = np.append(X_by_A[a_t - 1], np.transpose(c_t), axis = 0)\n",
    "                Y_by_A[a_t - 1] = np.append(Y_by_A[a_t - 1], np.array(R_lower).reshape(1, 1), axis = 0)\n",
    "                X_by_A[a_t - 1] = np.append(X_by_A[a_t - 1], np.transpose(c_t), axis = 0)\n",
    "\n",
    "            ## compute regret\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "    \n",
    "    else:\n",
    "        beta_est = np.zeros(d)\n",
    "        V_est = np.identity(d)*lam\n",
    "        Y = np.empty((0,1))\n",
    "        X = np.empty((0,d))\n",
    "        \n",
    "        # pull each arm once\n",
    "        for t in range(1, K+1):\n",
    "            a_t = t\n",
    "            c_K = env.get_context(t, True)\n",
    "            c_t = c_K[a_t - 1,:].reshape(d,1)\n",
    "            r_t = env.pull(a_t, t)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y = np.append(Y, np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X = np.append(X, np.array(c_t).reshape(1, d), axis = 0)\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "            \n",
    "        # GIRO loop\n",
    "        for t in range(K+1, n):\n",
    "            ## Boostrapping\n",
    "            s = len(Y)\n",
    "            idx = np.random.choice(s,s)\n",
    "            X_boot = X[idx,:]\n",
    "            Y_boot = Y[idx,:]\n",
    "            c_K = env.get_context(t, True)\n",
    "            V_est = np.linalg.inv(np.matmul(np.transpose(X_boot), X_boot) + np.identity(d)*lam)\n",
    "            beta_est = np.matmul(V_est, np.matmul(np.transpose(X_boot), Y_boot))\n",
    "            mu_est = np.matmul(c_K, beta_est).reshape(K)\n",
    "        \n",
    "            ## pull arm\n",
    "            a_t = np.argmax(mu_est) + 1\n",
    "            r_t = env.pull(a_t, t)\n",
    "            c_t = c_K[a_t - 1,:].reshape(d,1)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y = np.append(Y, np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X = np.append(X, np.array(c_t).reshape(1, d), axis = 0)\n",
    "            \n",
    "            ## pseudo rewards\n",
    "            for i in range(a):\n",
    "                Y = np.append(Y, np.array(R_upper).reshape(1, 1), axis = 0)\n",
    "                X = np.append(X, np.array(c_t).reshape(1, d), axis = 0)\n",
    "                Y = np.append(Y, np.array(R_lower).reshape(1, 1), axis = 0)\n",
    "                X = np.append(X, np.array(c_t).reshape(1, d), axis = 0)\n",
    "            \n",
    "            ## compute regret\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "        \n",
    "        \n",
    "    return R, A, regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_PHE(env, a, lam, R_upper, R_lower, coefficient_sharing = True):\n",
    "    '''\n",
    "    Perturbed-history exploratio Algorithm\n",
    "    This is a linear bandit algorithm\n",
    "    ============================================\n",
    "    INPUTS\n",
    "        env: stochastic bandit environment\n",
    "        a: Perturbation scale\n",
    "        lam: regularization parameter\n",
    "        R_upper: upper bound for reward  \n",
    "        R_lower: lower bound for reward\n",
    "        coefficient sharing: True if assuming true coefficient is same among arms\n",
    "    ============================================\n",
    "    OUPUTS \n",
    "        R: reward sequence, list with length n\n",
    "        A is action sequence, list with length n\n",
    "        regret: regret sequence, list with length n\n",
    "    ''' \n",
    "    # set up\n",
    "    n = env.n\n",
    "    K = env.k\n",
    "    d = env.d\n",
    "    lam = lam + 1e-20\n",
    "    regret = [0]\n",
    "    A = []\n",
    "    R = []\n",
    "    \n",
    "    if not coefficient_sharing:  #This is not correct\n",
    "        beta_est = [np.zeros(d) for i in range(K)]\n",
    "        V_est = [np.identity(d)*lam*(a + 1) for i in range(K)]\n",
    "        Y_by_A = [np.empty((0,1))]*K\n",
    "        X_by_A = [np.empty((0,d))]*K\n",
    "        arm_count = np.zeros(K)\n",
    "\n",
    "        # temporary liat/array\n",
    "        mu_est = np.zeros(K)\n",
    "        \n",
    "        # pull each arm once\n",
    "        for t in range(1, K+1):\n",
    "            a_t = t\n",
    "            c_t = env.get_context(t, False)\n",
    "            r_t = env.pull(a_t, t)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y_by_A[a_t - 1] = np.append(Y_by_A[a_t - 1], np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X_by_A[a_t - 1] = np.append(X_by_A[a_t - 1], np.array(c_t).reshape(1, d), axis = 0)\n",
    "            V_est[a_t - 1] = V_est[a_t - 1] + (a + 1) * np.matmul(c_t, c_t.T)\n",
    "            arm_count[a_t - 1] = arm_count[a_t  - 1] + 1\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "        \n",
    "        # PHE loop\n",
    "        for t in range(K+1, n):\n",
    "            ## pertubed Histroy\n",
    "            c_t = env.get_context(t, False)\n",
    "            c_t = np.array(c_t).reshape(d, 1)\n",
    "            U_sum = np.random.binomial(np.ceil(a * (t-1)).astype(int), 1/2)\n",
    "            U = np.random.multinomial(U_sum, [1.0 / (t-1)]*(t-1), size = 1)\n",
    "            Z_all = R_lower + (R_upper - R_lower) * U.reshape(t-1,1)\n",
    "            start = int(0)\n",
    "            for k in range(K):\n",
    "                X = X_by_A[k]\n",
    "                Y = Y_by_A[k]\n",
    "                s = arm_count[k].astype(int)\n",
    "                end = start + s\n",
    "                end = int(end)\n",
    "                Z = Z_all[start:end,0]\n",
    "                Z = Z.reshape(s, 1)\n",
    "                beta_est[k] = np.linalg.inv(V_est[k]) @ X.T @ (Y + Z)\n",
    "                mu_est[k] = np.matmul(c_t.T, beta_est[k].reshape(d,1))\n",
    "                start = int(start+arm_count[k])\n",
    "'''\n",
    "            for k in range(K):\n",
    "                X = X_by_A[k]\n",
    "                Y = Y_by_A[k]\n",
    "                s = arm_count[k].astype(int)\n",
    "                U_sum = np.random.binomial(np.ceil(a * s).astype(int), 1/2)\n",
    "                U = np.random.multinomial(U_sum, [1.0/s]*s, size = 1)\n",
    "                Z = R_lower + (R_upper - R_lower) * U.reshape(s,1)\n",
    "                beta_est[k] = np.linalg.inv(V_est[k]) @ X.T @ (Y + Z)\n",
    "                mu_est[k] = np.matmul(c_t.T, beta_est[k].reshape(d,1))\n",
    "'''\n",
    "            \n",
    "            ## pull arm\n",
    "            a_t = np.argmax(mu_est) + 1\n",
    "            r_t = env.pull(a_t, t)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y_by_A[a_t - 1] = np.append(Y_by_A[a_t - 1], np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X_by_A[a_t - 1] = np.append(X_by_A[a_t - 1], np.array(c_t).reshape(1, d), axis = 0)\n",
    "            \n",
    "            ## update\n",
    "            V_est[a_t - 1] = V_est[a_t - 1] + (a + 1) * np.matmul(c_t, c_t.T)\n",
    "            arm_count[a_t - 1] = arm_count[a_t  - 1] + 1\n",
    "            \n",
    "            ## compute regret\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)        \n",
    "    \n",
    "    else:    #This is correct\n",
    "        beta_est = np.zeros(d)\n",
    "        V_est = np.identity(d)*lam*(a + 1)\n",
    "        Y = np.empty((0,1))\n",
    "        X = np.empty((0,d))\n",
    "        \n",
    "        # pull each arm once\n",
    "        for t in range(1, K+1):\n",
    "            a_t = t\n",
    "            c_K = env.get_context(t, True)\n",
    "            c_t = c_K[a_t - 1,:].reshape(d,1)\n",
    "            r_t = env.pull(a_t, t)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y = np.append(Y, np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X = np.append(X, np.array(c_t).reshape(1, d), axis = 0)\n",
    "            V_est = V_est + (a + 1) * np.matmul(c_t, c_t.T)\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "        \n",
    "        # PHE loop\n",
    "        for t in range(K+1, n):\n",
    "            ## pertubed Histroy\n",
    "            c_K = env.get_context(t, True)\n",
    "            U_sum = np.random.binomial(np.ceil(a * (t-1)).astype(int), 1/2)\n",
    "            U = np.random.multinomial(U_sum, [1.0 / (t-1)]*(t-1), size = 1)\n",
    "            Z = R_lower + (R_upper - R_lower) * U.reshape(t-1,1)\n",
    "            beta_est = np.linalg.inv(V_est) @ X.T @ (Y + Z)\n",
    "            mu_est = np.matmul(c_K, beta_est).reshape(K)\n",
    "        \n",
    "            ## pull arm\n",
    "            a_t = np.argmax(mu_est) + 1\n",
    "            r_t = env.pull(a_t, t)\n",
    "            c_t = c_K[a_t - 1,:].reshape(d,1)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y = np.append(Y, np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X = np.append(X, np.array(c_t).reshape(1, d), axis = 0)\n",
    "            \n",
    "            ## update\n",
    "            V_est = V_est + (a + 1) * np.matmul(c_t, c_t.T)\n",
    "            \n",
    "            ## compute regret\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "            \n",
    "            \n",
    "    return R, A, regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_UCB(env, lam, delta = 0.05, coefficient_sharing = True):\n",
    "    '''\n",
    "    Upper Confidence Bound Algorithm\n",
    "    This is a linear bandit algorithm\n",
    "    ============================================\n",
    "    INPUTS\n",
    "        env: stochastic bandit environment\n",
    "        lam: regularization parameter\n",
    "        delta: tolarence, 1-delta is confidence level\n",
    "        coefficient sharing: True if assuming true coefficient is same among arms\n",
    "    ============================================\n",
    "    OUPUTS \n",
    "        R: reward sequence, list with length n\n",
    "        A is action sequence, list with length n\n",
    "        regret: regret sequence, list with length n\n",
    "    '''\n",
    "    # set up\n",
    "    n = env.n\n",
    "    K = env.k\n",
    "    d = env.d\n",
    "    lam = lam + 1e-20\n",
    "    regret = [0]\n",
    "    A = []\n",
    "    R = []\n",
    "    \n",
    "    if not coefficient_sharing:     # this is correct\n",
    "        beta_est = [np.zeros(d) for i in range(K)]\n",
    "        V_est = [np.identity(d)*lam for i in range(K)]\n",
    "        Sigma_est = [np.identity(d)*(1/lam) for i in range(K)]\n",
    "        g = [np.zeros(d).reshape(d,1) for i in range(K)]\n",
    "        Y_by_A = [np.empty((0,1))]*K\n",
    "        X_by_A = [np.empty((0,d))]*K\n",
    "\n",
    "        # temporary liat/array\n",
    "        ucb = np.zeros(K)\n",
    "        radius = np.zeros(K)\n",
    "        beta_norm_max = np.zeros(K)\n",
    "        V_det = np.zeros(K)\n",
    "\n",
    "        # pull each arm once\n",
    "        for t in range(1, K+1):\n",
    "            c_t = env.get_context(t, False)\n",
    "            a_t = t\n",
    "            r_t = env.pull(a_t, t)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y_by_A[a_t - 1] = np.append(Y_by_A[a_t - 1], np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X_by_A[a_t - 1] = np.append(X_by_A[a_t - 1], np.array(c_t).reshape(1, d), axis = 0)\n",
    "            V_est[a_t - 1] = V_est[a_t - 1] + np.matmul(c_t, c_t.T)\n",
    "            Sigma_est[a_t - 1] = np.linalg.inv(V_est[a_t - 1])\n",
    "            g[a_t - 1] = g[a_t - 1] + r_t*c_t.reshape(d,1)\n",
    "            beta_est[a_t - 1] = Sigma_est[a_t - 1] @ g[a_t - 1]\n",
    "            V_det[a_t - 1] = np.linalg.det(V_est[a_t - 1])\n",
    "            beta_norm_tmp = np.linalg.norm(beta_est[a_t - 1])\n",
    "            if beta_norm_tmp > beta_norm_max[a_t - 1]:\n",
    "                beta_norm_max[a_t - 1] = beta_norm_tmp\n",
    "            radius[a_t - 1] = np.sqrt(lam)*beta_norm_max[a_t - 1] + np.sqrt(2*np.log(1/delta) + np.log(V_det[a_t - 1]/lam**d))\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "\n",
    "        # UCB loop\n",
    "        for t in range(K+1, n):\n",
    "            c_t = env.get_context(t, False)\n",
    "            for k in range(K):\n",
    "                ucb[k] = c_t.T @ beta_est[k] + radius[k] * np.sqrt(c_t.T @ Sigma_est[k] @ c_t)\n",
    "\n",
    "            ## pull arm\n",
    "            a_t = np.argmax(ucb) + 1\n",
    "            r_t = env.pull(a_t, t)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y_by_A[a_t - 1] = np.append(Y_by_A[a_t - 1], np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X_by_A[a_t - 1] = np.append(X_by_A[a_t - 1], np.array(c_t).reshape(1, d), axis = 0)\n",
    "\n",
    "            ## least square update\n",
    "            V_est[a_t - 1] = V_est[a_t - 1] + np.matmul(c_t, c_t.T)\n",
    "            Sigma_est[a_t - 1] = np.linalg.inv(V_est[a_t - 1])\n",
    "            g[a_t - 1] = g[a_t - 1] + r_t*c_t.reshape(d,1)\n",
    "            beta_est[a_t - 1] = Sigma_est[a_t - 1] @ g[a_t - 1]\n",
    "\n",
    "            ## confidence ellipsoid update \n",
    "            V_det[a_t - 1] = np.linalg.det(V_est[a_t - 1])\n",
    "            beta_norm_tmp = np.linalg.norm(beta_est[a_t - 1])\n",
    "            if beta_norm_tmp > beta_norm_max[a_t - 1]:\n",
    "                beta_norm_max[a_t - 1] = beta_norm_tmp\n",
    "            radius[a_t - 1] = np.sqrt(lam)*beta_norm_max[a_t - 1] + np.sqrt(2*np.log(1/delta) + np.log(V_det[a_t - 1]/lam**d))\n",
    "\n",
    "            ## compute regret\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t) \n",
    "    \n",
    "    else:     \n",
    "        beta_est = np.zeros(d)\n",
    "        V_est = np.identity(d)*lam\n",
    "        Sigma_est = np.identity(d)*(1/lam)\n",
    "        g = np.zeros(d).reshape(d,1)\n",
    "        Y = np.empty((0,1))\n",
    "        X = np.empty((0,d))\n",
    "        \n",
    "        # temporary liat/array\n",
    "        ucb = np.zeros(K)\n",
    "        radius = np.zeros(1)\n",
    "        beta_norm_max = np.zeros(1)\n",
    "        V_det = np.zeros(1)\n",
    "        \n",
    "        # pull each arm once\n",
    "        for t in range(1, K+1):\n",
    "            a_t = t\n",
    "            c_K = env.get_context(t, True)\n",
    "            c_t = c_K[a_t - 1,:].reshape(d,1)\n",
    "            r_t = env.pull(a_t, t)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y = np.append(Y, np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X = np.append(X, np.array(c_t).reshape(1, d), axis = 0)\n",
    "            V_est = V_est + np.matmul(c_t, c_t.T)\n",
    "            Sigma_est = np.linalg.inv(V_est)\n",
    "            g = g + r_t*c_t.reshape(d,1)\n",
    "            beta_est = Sigma_est @ g\n",
    "            V_det = np.linalg.det(V_est)\n",
    "            beta_norm_tmp = np.linalg.norm(beta_est)\n",
    "            if beta_norm_tmp > beta_norm_max:\n",
    "                beta_norm_max = beta_norm_tmp\n",
    "            radius = np.sqrt(lam)*beta_norm_max + np.sqrt(2*np.log(1/delta) + np.log(V_det/lam**d))\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "        \n",
    "        # UCB loop\n",
    "        for t in range(K+1, n):\n",
    "            c_K = env.get_context(t, True)\n",
    "            for k in range(K):\n",
    "                c_k = c_K[k,:].reshape(d,1)\n",
    "                ucb[k] = c_k.T @ beta_est + radius * np.sqrt(c_k.T @ Sigma_est @ c_k)\n",
    "\n",
    "            ## pull arm\n",
    "            a_t = np.argmax(ucb) + 1\n",
    "            r_t = env.pull(a_t, t)\n",
    "            c_t = c_K[a_t - 1,:].reshape(d,1)\n",
    "            A.append(a_t)\n",
    "            R.append(r_t)\n",
    "            Y = np.append(Y, np.array(r_t).reshape(1, 1), axis = 0)\n",
    "            X = np.append(X, np.array(c_t).reshape(1, d), axis = 0)\n",
    "\n",
    "            ## least square update\n",
    "            V_est = V_est + np.matmul(c_t, c_t.T)\n",
    "            Sigma_est = np.linalg.inv(V_est)\n",
    "            g = g + r_t*c_t.reshape(d,1)\n",
    "            beta_est = Sigma_est @ g\n",
    "\n",
    "            ## confidence ellipsoid update \n",
    "            V_det = np.linalg.det(V_est)\n",
    "            beta_norm_tmp = np.linalg.norm(beta_est)\n",
    "            if beta_norm_tmp > beta_norm_max:\n",
    "                beta_norm_max = beta_norm_tmp\n",
    "            radius = np.sqrt(lam)*beta_norm_max + np.sqrt(2*np.log(1/delta) + np.log(V_det/lam**d))\n",
    "            \n",
    "            ## compute regret\n",
    "            regret_t = regret[t - 1] + env.regret(a_t, t)\n",
    "            regret.append(regret_t)\n",
    "        \n",
    "    return R, A, regret       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Uniform_constrained_context(k, d, min = 0, max = 1):\n",
    "    '''\n",
    "    function to generate d-diemsnional context from uniform distribution such that\n",
    "    norms are 1-(k-1)/k^2, 1-(k-2)/k^2, ..., 1-1/k^2, 1\n",
    "    ============================================\n",
    "    INPUT\n",
    "        k: number of arms\n",
    "        d: dimension of context, defualt is 2d context\n",
    "        min: lower bound for uniform distribution, default is 0\n",
    "        max: upper bound for uniform distribution, default is 1\n",
    "    ============================================\n",
    "    OUPUT\n",
    "        c: contexts for all arms, k by d numpy array\n",
    "    '''\n",
    "    c = np.zeros(k*d).reshape(k,d)\n",
    "    for i in range(k):\n",
    "        c_k = np.random.uniform(low = min, high = max, size = d).reshape(d) \n",
    "        norm = np.linalg.norm(c_k)\n",
    "        c_k = c_k / norm\n",
    "        c[i,:] = c_k\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_len = 1000\n",
    "k = 100\n",
    "d = 5\n",
    "beta = np.random.uniform(-0.5, 0.5, 1*d).reshape(1, d)\n",
    "beta = beta/np.linalg.norm(beta)\n",
    "sigma_seq = np.ones(k) * np.sqrt(0.1)\n",
    "nu = [np.random.uniform(0, 1, 1*d).reshape(d)]*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_constrained_context(k, d, mean = nu):\n",
    "    '''\n",
    "    function to generate d-diemsnional context from Gaussian distribution\n",
    "    such that norms are 1\n",
    "    ============================================\n",
    "    INPUT\n",
    "        k: number of arms\n",
    "        d: dimension of context, defualt is 2d context\n",
    "        mean: mean for d-dimensional Gasussian\n",
    "    ============================================\n",
    "    OUPUT\n",
    "        c: contexts for all arms, k by d numpy array\n",
    "    '''\n",
    "    cov = [1/(2*k)*np.identity(d)]*k\n",
    "    c = np.zeros(k*d).reshape(k,d)\n",
    "    for i in range(k):\n",
    "        c_k = np.array(stats.multivariate_normal.rvs(mean = mean[i], cov = cov[i], size = 1)).reshape(d) \n",
    "        norm = np.linalg.norm(c_k)\n",
    "        c_k = c_k / norm\n",
    "        c[i,:] = c_k\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Linear_Bandit(k = k, n = horizon_len, beta = beta, sigma = sigma_seq, random_context = True, \n",
    "                    gen_context = Gaussian_constrained_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_design(k, d):\n",
    "    '''\n",
    "    function to beta for linear bandit with covariates setting\n",
    "    ============================================\n",
    "    INPUT\n",
    "        k: number of arms\n",
    "        d: dimension of context\n",
    "    ============================================\n",
    "    OUPUT\n",
    "        beta: true parameter, k by d\n",
    "    '''\n",
    "    beta = np.ones(k*d).reshape(k,d)\n",
    "    idx = []\n",
    "    for kk in range(k):\n",
    "        b = np.random.binomial(d, 1/2)\n",
    "        idx_k = np.random.choice(d, b, replace = False)\n",
    "        beta[kk,:][idx_k] = -1\n",
    "        beta[kk,:] = beta[kk,:] + np.random.uniform(low = -0.95, high = 0.95, size = d)\n",
    "        beta[kk,:] = (kk + 1)/k * beta[kk,:]/np.linalg.norm(beta[kk,:])\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
